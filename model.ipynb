{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea232d-1e93-4c63-89e6-32a28ae51fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import pandas_ta\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535dbcd-68ec-4aa5-9581-cbb1fb85198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "# Some data cleaning as some of the symbols contain '.' and AMTM, CTAS stock possibly delisted\n",
    "sp500['Symbol'] = sp500['Symbol'].str.replace('.','-')\n",
    "symbol_list = sp500['Symbol'].unique().tolist()\n",
    "for s in ['AMTM', 'CTAS']:\n",
    "    symbol_list.remove(s)\n",
    "end_date = '2024-09-30'\n",
    "interval_size = 20 # in years \n",
    "start_date = pd.to_datetime(end_date) - pd.DateOffset(365*interval_size)\n",
    "\n",
    "df = yf.download(tickers=symbol_list, start=start_date, end=end_date).stack()\n",
    "df.index.names = ['date', 'ticker']\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb94a83-19c6-4172-9f4d-b20144b82276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.tz_localize(None, level='date') # remove timezone from under the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d828d3-6bc8-48b0-b706-719e2c53ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['garman-klass'] = ((np.log(df['high'])-np.log(df['low']))**2)/2 - (2*np.log(2)-1)*((np.log(df['adj close'])-np.log(df['open'])))**2\n",
    "# level 1 is the ticker level; RSI is the only indicator we won't normalise\n",
    "df['rsi'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.rsi(close=x, length=20))\n",
    "# Use log normalisation when calculating Bollinger bands\n",
    "df['bb_low'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,0])\n",
    "df['bb_mid'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,1])\n",
    "df['bb_high'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,2])\n",
    "# Pandas TA's ATR takes 3 columns as input so we can't pass it into transform()\n",
    "# We thus compute ATR using a custom function\n",
    "def compute_atr(stock_data):\n",
    "    atr = pandas_ta.atr(high=stock_data['high'], \n",
    "                        low=stock_data['low'], \n",
    "                        close=stock_data['close'], \n",
    "                        length=14)\n",
    "    return atr.sub(atr.mean()).div(atr.std()) # returns ATR indicator normalised for each stock\n",
    "\n",
    "df['atr'] = df.groupby(level=1, group_keys=False).apply(compute_atr)\n",
    "# We also compute MACD using a custom function\n",
    "def compute_macd(close):\n",
    "    macd = pandas_ta.macd(close=close, length=20).iloc[:,0]\n",
    "    return macd.sub(macd.mean()).div(macd.std()) # again, normalise before returning\n",
    "\n",
    "df['macd'] = df.groupby(level=1, group_keys=False)['adj close'].apply(compute_macd)\n",
    "# Dollar volume liquidity per million\n",
    "df['dollar_volume'] = (df['adj close']*df['volume'])*1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b67dfb-08e5-4901-8bbc-a4c8801783a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols = [c for c in df.columns.unique(0) if c not in ['dollar_volume', 'volume', 'open', 'high', 'low', 'close']]\n",
    "data = (pd.concat([df.unstack('ticker')['dollar_volume'].resample('M').mean().stack('ticker').to_frame('dollar_volume'),\n",
    "           df.unstack()[last_cols].resample('M').last().stack('ticker')],\n",
    "         axis=1)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dfee13-16bd-4b25-9cd1-c8f57fdf86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dollar_volume'] = (data['dollar_volume'].unstack('ticker').rolling(5*12).mean().stack())\n",
    "data['dollar_volume_rank'] = (data.groupby('date')['dollar_volume'].rank(ascending=False))\n",
    "data = data[data['dollar_volume_rank']<150].drop(['dollar_volume', 'dollar_volume_rank'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7546a-c0cd-408a-9d84-25d864be2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(df):\n",
    "    outlier_cutoff = 0.005 # 99.5 percentile is our outlier cutoff\n",
    "    lags = [1, 2, 3, 6, 9, 12]\n",
    "    for lag in lags:\n",
    "        df[f'return_{lag}m'] = (df['adj close']\n",
    "                               .pct_change(lag)\n",
    "                               .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoff),\n",
    "                                                      upper=x.quantile(1-outlier_cutoff)))\n",
    "                               .add(1).pow(1/lag).sub(1))\n",
    "    return df\n",
    "\n",
    "\n",
    "data = data.groupby(level=1, group_keys=False).apply(compute_returns).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d0184-1d51-45eb-be9e-a01f55398d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3',\n",
    "                               'famafrench',\n",
    "                               start='2010')[0].drop('RF', axis=1)\n",
    "\n",
    "factor_data.index = factor_data.index.to_timestamp()\n",
    "\n",
    "factor_data = factor_data.resample('M').last().div(100)\n",
    "\n",
    "factor_data.index.name = 'date'\n",
    "\n",
    "factor_data = factor_data.join(data['return_1m']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534119f0-0cae-4c0f-8184-87c60a6b80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = factor_data.groupby(level=1).size()\n",
    "\n",
    "valid_stocks = observations[observations >= 10]\n",
    "\n",
    "factor_data = factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af539b-95be-4734-96ca-4dae302d4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = (factor_data.groupby(level=1,\n",
    "                            group_keys=False)\n",
    "         .apply(lambda x: RollingOLS(endog=x['return_1m'], \n",
    "                                     exog=sm.add_constant(x.drop('return_1m', axis=1)),\n",
    "                                     window=min(24, x.shape[0]),\n",
    "                                     min_nobs=len(x.columns)+1)\n",
    "         .fit(params_only=True)\n",
    "         .params\n",
    "         .drop('const', axis=1)))\n",
    "\n",
    "\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "data = (data.join(betas.groupby('ticker').shift()))\n",
    "\n",
    "data.loc[:, factors] = data.groupby('ticker', group_keys=False)[factors].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "data = data.drop('adj close', axis=1)\n",
    "\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141093c1-f2ca-40d6-b79e-919501810b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rsi_values = [30, 45, 55, 75]\n",
    "initial_centroids = np.zeros((len(target_rsi_values), 18))\n",
    "initial_centroids[:,1] = target_rsi_values\n",
    "initial_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb9daa-3952-47b2-89d2-a2f07ea07458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_epochs = 50\n",
    "def get_clusters(df):\n",
    "    df['cluster'] = KMeans(n_clusters=4,\n",
    "                           random_state=0,\n",
    "                           init=initial_centroids).fit(df).labels_\n",
    "    return df\n",
    "\n",
    "for i in range(0,n_epochs):\n",
    "    data = data.dropna().groupby('date', group_keys=False).apply(get_clusters)\n",
    "    if i<n_epochs-1:\n",
    "        data = data.drop('cluster', axis=1)\n",
    "\n",
    "def plot_clusters(data, filename):\n",
    "\n",
    "    cluster_0 = data[data['cluster']==0]\n",
    "    cluster_1 = data[data['cluster']==1]\n",
    "    cluster_2 = data[data['cluster']==2]\n",
    "    cluster_3 = data[data['cluster']==3]\n",
    "\n",
    "    plt.scatter(cluster_0.iloc[:,5] , cluster_0.iloc[:,1] , color = 'red', label='cluster 0')\n",
    "    plt.scatter(cluster_1.iloc[:,5] , cluster_1.iloc[:,1] , color = 'green', label='cluster 1')\n",
    "    plt.scatter(cluster_2.iloc[:,5] , cluster_2.iloc[:,1] , color = 'blue', label='cluster 2')\n",
    "    plt.scatter(cluster_3.iloc[:,5] , cluster_3.iloc[:,1] , color = 'black', label='cluster 3')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(filename+'.jpg')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "for i in data.index.get_level_values('date').unique().tolist():\n",
    "    g = data.xs(i, level=0)\n",
    "    s = f'{i}'.replace(\" 00:00:00\", \"\")\n",
    "    plt.title(s)\n",
    "    plot_clusters(g, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b5ad7-b823-4337-b616-8bc29d9f935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "\n",
    "filtered_df = data[data['cluster']==3].copy()\n",
    "filtered_df = filtered_df.reset_index(level=1)\n",
    "filtered_df.index = filtered_df.index+pd.DateOffset(1)\n",
    "filtered_df = filtered_df.reset_index().set_index(['date', 'ticker'])\n",
    "dates = filtered_df.index.get_level_values('date').unique().tolist()\n",
    "fixed_dates = {}\n",
    "\n",
    "for d in dates:\n",
    "    \n",
    "    fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d, level=0).index.tolist()\n",
    "    \n",
    "def optimize_weights(prices, lower_bound=0):\n",
    "    \n",
    "    returns = expected_returns.mean_historical_return(prices=prices,\n",
    "                                                      frequency=252)\n",
    "    \n",
    "    cov = risk_models.sample_cov(prices=prices,\n",
    "                                 frequency=252)\n",
    "    \n",
    "    ef = EfficientFrontier(expected_returns=returns,\n",
    "                           cov_matrix=cov,\n",
    "                           weight_bounds=(lower_bound, .1),\n",
    "                           solver='SCS')\n",
    "    \n",
    "    weights = ef.max_sharpe()\n",
    "    \n",
    "    return ef.clean_weights()\n",
    "\n",
    "\n",
    "# Download fresh stocks data for shortlisted stocks\n",
    "stocks = data.index.get_level_values('ticker').unique().tolist()\n",
    "\n",
    "new_df = yf.download(tickers=stocks,\n",
    "                     start=data.index.get_level_values('date').unique()[0]-pd.DateOffset(months=12),\n",
    "                     end=data.index.get_level_values('date').unique()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa1955-e69e-4ffc-ae1b-f3509c3aae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_dataframe = np.log(new_df['Adj Close']).diff()\n",
    "\n",
    "portfolio_df = pd.DataFrame()\n",
    "\n",
    "for start_date in fixed_dates.keys():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        end_date = (pd.to_datetime(start_date)+pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')\n",
    "\n",
    "        cols = fixed_dates[start_date]\n",
    "\n",
    "        optimization_start_date = (pd.to_datetime(start_date)-pd.DateOffset(months=12)).strftime('%Y-%m-%d')\n",
    "\n",
    "        optimization_end_date = (pd.to_datetime(start_date)-pd.DateOffset(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        optimization_df = new_df['Adj Close'][optimization_start_date:optimization_end_date][cols]\n",
    "        \n",
    "        success = False\n",
    "        try:\n",
    "            weights = optimize_weights(prices=optimization_df,\n",
    "                                   lower_bound=round(1/(len(optimization_df.columns)*2),3))\n",
    "\n",
    "            weights = pd.DataFrame(weights, index=pd.Series(0))\n",
    "            \n",
    "            success = True\n",
    "        except:\n",
    "            print(f'Max Sharpe Optimization failed for {start_date}, Continuing with Equal-Weights')\n",
    "        \n",
    "        if success==False:\n",
    "            weights = pd.DataFrame([1/len(optimization_df.columns) for i in range(len(optimization_df.columns))],\n",
    "                                     index=optimization_df.columns.tolist(),\n",
    "                                     columns=pd.Series(0)).T\n",
    "        \n",
    "        temp_df = returns_dataframe[start_date:end_date]\n",
    "\n",
    "        temp_df = temp_df.stack().to_frame('return').reset_index(level=0)\\\n",
    "                   .merge(weights.stack().to_frame('weight').reset_index(level=0, drop=True),\n",
    "                          left_index=True,\n",
    "                          right_index=True)\\\n",
    "                   .reset_index().set_index(['Date', 'Ticker']).unstack().stack()\n",
    "\n",
    "        temp_df.index.names = ['date', 'ticker']\n",
    "\n",
    "        temp_df['weighted_return'] = temp_df['return']*temp_df['weight']\n",
    "\n",
    "        temp_df = temp_df.groupby(level=0)['weighted_return'].sum().to_frame('Strategy Return')\n",
    "\n",
    "        portfolio_df = pd.concat([portfolio_df, temp_df], axis=0)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "portfolio_df = portfolio_df.drop_duplicates()\n",
    "portfolio_df = portfolio_df.tz_localize(None, level='date') # remove timezone from under the date\n",
    "portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8dc0cf-05ec-41a6-ac68-3bf3069cd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy = yf.download(tickers='SPY',\n",
    "                  start='2010-11-01',\n",
    "                  end=dt.date.today())\n",
    "\n",
    "spy_ret = np.log(spy[['Adj Close']]).diff().dropna().rename({'Adj Close':'SPY Buy&Hold'}, axis=1)\n",
    "\n",
    "portfolio_df = portfolio_df.merge(spy_ret,\n",
    "                                  left_index=True,\n",
    "                                  right_index=True)\n",
    "portfolio_df['Adjusted Return'] = portfolio_df['Strategy Return'] - portfolio_df['SPY Buy&Hold']\n",
    "portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f484120-0338-4038-a32a-f16918c0c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "portfolio_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum())-1\n",
    "\n",
    "portfolio_cumulative_return[:end_date].plot(figsize=(16,6))\n",
    "\n",
    "plt.title('Unsupervised Learning Trading Strategy Returns Over Time')\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "plt.ylabel('Returns')\n",
    "\n",
    "plt.savefig('returns.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc714e75-02b2-4874-ba3a-6588cb6cb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "portfolio_cumulative_return = np.exp(np.log1p(portfolio_df['Adjusted Return']).cumsum())-1\n",
    "\n",
    "portfolio_cumulative_return[:end_date].plot(figsize=(16,6))\n",
    "\n",
    "plt.title('Adjusted Strategy Returns Over Time')\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "plt.ylabel('Adjusted Returns')\n",
    "\n",
    "plt.savefig('adjusted-returns.jpg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
